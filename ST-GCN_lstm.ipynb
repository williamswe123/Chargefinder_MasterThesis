{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57594bf-9c73-4291-b818-9e1ad179aee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bce56d-8cda-4a98-805b-b629516a6f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import copy\n",
    "import random\n",
    "\n",
    "import networkx as nx\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Batch, Data, Dataset\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import time\n",
    "\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb8792b-1c1e-4f2c-9705-a1b1ae361d44",
   "metadata": {},
   "source": [
    "# Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d92c905-ae36-4074-8492-1adac8389c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\"donken\",\n",
    "         \"Holmgrens\",\n",
    "         \"IONITY\",\n",
    "         \"Jureskogs_Vattenfall\",\n",
    "         \"UFC\"]\n",
    "\n",
    "nr_of_data_points = 163618\n",
    "splits=[0.8, 0.9]\n",
    "\n",
    "data_dict = {}\n",
    "\n",
    "\n",
    "for f in files:\n",
    "    data = pd.read_csv('data/varnamo/data_' + f + '_5T_k-10.csv')\n",
    "    #data.info()\n",
    "    data.set_index('Unnamed: 0', inplace=True)\n",
    "    \n",
    "    data = data.drop(columns=data.columns.difference(['Occupancy']))\n",
    "    \n",
    "    data_dict[f] = data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20367ea8-15e9-4025-b27a-84fd7a3de352",
   "metadata": {},
   "source": [
    "# Model Making"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e160da-671d-4715-9a63-0537a8e7d6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add nodes\n",
    "num_nodes = 5\n",
    "G.add_nodes_from(range(num_nodes))\n",
    "\n",
    "Jureskogs_Vattenfall = 0\n",
    "IONITY = 1\n",
    "donken = 2\n",
    "Holmgrens = 3\n",
    "UFC = 4\n",
    "\n",
    "# Define edges to connect specific nodes with custom weights\n",
    "edges_to_connect = [\n",
    "    (Jureskogs_Vattenfall, IONITY, 230),\n",
    "    (Jureskogs_Vattenfall, UFC, 750),\n",
    "    (Jureskogs_Vattenfall, Holmgrens, 750),\n",
    "    (Jureskogs_Vattenfall, donken, 650),\n",
    "    (IONITY, UFC, 550),\n",
    "    (IONITY, Holmgrens, 500),\n",
    "    (IONITY, donken, 450),\n",
    "    (UFC, Holmgrens, 280),\n",
    "    (UFC, donken, 550),\n",
    "    (Holmgrens, donken, 550)\n",
    "]\n",
    "\n",
    "# Add edges with custom weights\n",
    "for edge in edges_to_connect:\n",
    "    G.add_edge(edge[0], edge[1], weight=edge[2])\n",
    "\n",
    "# Create adjacency matrix with weights\n",
    "adj_matrix = nx.adjacency_matrix(G).todense()\n",
    "\n",
    "torch_adj_matrix = torch.Tensor(adj_matrix)\n",
    "\n",
    "edge_index = torch_adj_matrix.nonzero(as_tuple=False).t().contiguous()\n",
    "edge_attr = torch_adj_matrix[torch_adj_matrix.nonzero()].reshape(-1, 1)\n",
    "\n",
    "\n",
    "# Define colors for nodes\n",
    "node_colors = ['Yellow'] * num_nodes\n",
    "\n",
    "print(G.nodes)\n",
    "\n",
    "# Draw the graph\n",
    "pos = nx.spring_layout(G)  # positions for all nodes\n",
    "nx.draw(G, pos, with_labels=True, node_color=node_colors, node_size=700, font_size=10)\n",
    "edge_labels = nx.get_edge_attributes(G, 'weight')\n",
    "nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels)\n",
    "\n",
    "# Show the graph\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ac3e2e-f9b7-4995-8f25-a26eb7ea7642",
   "metadata": {},
   "outputs": [],
   "source": [
    "future_steps = 36\n",
    "seq_len = 576\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "indices = list(range(len(data) - future_steps - seq_len))\n",
    "print(len(indices))\n",
    "random.shuffle(indices)\n",
    "print(len(indices))\n",
    "\n",
    "train_i = indices[:int(len(indices)*0.8)] \n",
    "val_i = indices[int(len(indices)*0.8):int(len(indices)*0.9)]\n",
    "test_i = indices[int(len(indices)*0.9):]\n",
    "\n",
    "\n",
    "class datasetMaker(Dataset):\n",
    "    def __init__(self, station_data, indices_conversion, edge_index, edge_attr, seq_len, future_steps, batch_size):\n",
    "        self.station_data = station_data\n",
    "        self.indices_conversion = indices_conversion\n",
    "        self.size = station_data[\"donken\"].shape[0]\n",
    "        self.edge_index = edge_index\n",
    "        self.edge_attr = edge_attr\n",
    "        self.seq_len = seq_len\n",
    "        self.future_steps = future_steps\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices_conversion) - self.seq_len - self.future_steps\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        index = self.indices_conversion[index]\n",
    "        \n",
    "        seq_end = index + self.seq_len\n",
    "        fut_end = index + self.seq_len + self.future_steps\n",
    "        \n",
    "        node_features = []\n",
    "        for i, (station, data) in enumerate(self.station_data.items()):\n",
    "            node_feature = data.iloc[index:seq_end].values\n",
    "            node_features.append(node_feature)\n",
    "        node_features = torch.tensor(np.array(node_features)).float()\n",
    "\n",
    "        labels = []\n",
    "        for i, (station, data) in enumerate(self.station_data.items()):\n",
    "            label = data.iloc[seq_end:fut_end].values\n",
    "            labels.append(label)\n",
    "        labels = torch.unsqueeze(torch.tensor(np.array(labels)), dim=2).float()\n",
    "        \n",
    "        Gdata = Data(x=node_features, y=labels, edge_index=self.edge_index, edge_attr=self.edge_attr)\n",
    "\n",
    "        return Gdata, labels\n",
    "    \n",
    "def custom_collate(batch):\n",
    "    label = torch.cat([i[1] for i in batch])\n",
    "    \n",
    "    label = label.squeeze(3)\n",
    "    \n",
    "    batch = Batch.from_data_list([b[0] for b in batch])\n",
    "    \n",
    "    return batch, label\n",
    "\n",
    "train_dataset = datasetMaker(data_dict, train_i, edge_index, edge_attr, seq_len, future_steps, batch_size)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, drop_last=True, collate_fn=custom_collate)\n",
    "\n",
    "val_dataset = datasetMaker(data_dict, val_i, edge_index, edge_attr, seq_len, future_steps, batch_size)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=True, drop_last=True, collate_fn=custom_collate)\n",
    "\n",
    "test_dataset = datasetMaker(data_dict, test_i, edge_index, edge_attr, seq_len, future_steps, batch_size)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True, drop_last=True, collate_fn=custom_collate)\n",
    "\n",
    "\n",
    "print(\"train len \", len(train_loader))\n",
    "print(\"val len   \", len(val_loader))\n",
    "print(\"test len  \", len(test_loader))\n",
    "\n",
    "\n",
    "for data, label in train_loader:\n",
    "    print(data)\n",
    "    print(label.shape)\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a18916-815d-468a-bf0c-b2a7871f5de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def reshape_to_batches(x, batch_description):\n",
    "    \"\"\"\n",
    "        Does something like this:\n",
    "        torch.Size([28, 576, 64]) --> torch.Size([4, 7, 576, 64])\n",
    "    \"\"\"\n",
    "    num_splits = batch_description.max().item() + 1\n",
    "    new_shape_dim_0 = num_splits\n",
    "    new_shape_dim_1 = x.size(0) // new_shape_dim_0\n",
    "    new_shape = torch.Size([new_shape_dim_0, new_shape_dim_1] + list(x.size()[1:]))\n",
    "    reshaped_tensor = x.view(new_shape)\n",
    "    return reshaped_tensor\n",
    "\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels=1, gcn_hidden_channels=8, gcn_layers=1):\n",
    "        super(GCN, self).__init__()\n",
    "        self.in_conv = GCNConv(in_channels, gcn_hidden_channels)\n",
    "        self.hidden_convs = [GCNConv(gcn_hidden_channels, gcn_hidden_channels).cuda() for i in range(gcn_layers - 1)]\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = x.float()\n",
    "        x = self.in_conv(x, edge_index)\n",
    "        for conv in self.hidden_convs:\n",
    "            x = F.relu(x)\n",
    "            x = conv(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        return x\n",
    "\n",
    "class MultiStepLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
    "        super(MultiStepLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "        # Flatten LSTM parameters\n",
    "        self.lstm.flatten_parameters()\n",
    "        \n",
    "        # Fully connected layer to map LSTM output to desired output_size\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x, future=1):\n",
    "        \n",
    "        predictions = []\n",
    "        \n",
    "        \n",
    "        for _ in range(future):\n",
    "            # Initialize hidden state and cell state        \n",
    "            batch_size, sequence_length, _ = x.size()\n",
    "            h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)\n",
    "            c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)\n",
    "\n",
    "            # LSTM forward pass\n",
    "            out, (h0, c0) = self.lstm(x, (h0, c0))\n",
    "            \n",
    "            pred = out[:, -1, :]\n",
    "            \n",
    "            x = torch.cat((x, pred.unsqueeze(1)), dim=1)\n",
    "            \n",
    "            t = self.fc(pred)\n",
    "            predictions.append(t) # Append occupancy to predictions\n",
    "            \n",
    "\n",
    "        # Stack predictions along the sequence length dimension'\n",
    "        predictions = torch.cat(predictions, dim=1)\n",
    "        \n",
    "        predictions = torch.unsqueeze(predictions, dim = 2)\n",
    "        return predictions\n",
    "    \n",
    "class STGCN(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, gcn_layers, hidden_channels, lstm_layers, out_channels):\n",
    "        super(STGCN, self).__init__()\n",
    "        \n",
    "        print(\"\\033[100mhidden_channels:\", hidden_channels,\n",
    "              \"   GCN hidden layers:\", gcn_layers,\n",
    "              \"   lstm_layers:\", lstm_layers, \"\\033[0m\")\n",
    "\n",
    "        \n",
    "        self.GCN = GCN(in_channels=in_channels, gcn_hidden_channels=hidden_channels, gcn_layers=gcn_layers)\n",
    "\n",
    "        self.lstm = MultiStepLSTM(hidden_channels, hidden_channels, out_channels, lstm_layers)\n",
    "        \n",
    "    def forward(self, data):    \n",
    " \n",
    "        batch = data.batch\n",
    "        \n",
    "        data.x = data.x.float()  # Convert node features to Double\n",
    "        data.edge_attr = data.edge_attr.float()  # Convert edge attributes to Double\n",
    "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
    "       \n",
    "        # Spatial processing\n",
    "        x = self.GCN(x, edge_index, edge_attr)\n",
    "\n",
    "        x = reshape_to_batches(x, batch)\n",
    "        # Reshape and pass data through the model for each station\n",
    "        predictions = []\n",
    "        for station_data in x.permute(1,0,2,3):  # Iterate over each station\n",
    "            #station_data = station_data.permute(1, 0, 2)  # Reshape for LSTM (batch_first=True)\n",
    "            output = self.lstm(station_data, future=future_steps)\n",
    "            predictions.append(output)\n",
    "\n",
    "        # Concatenate predictions for all stations\n",
    "        predictions = torch.stack(predictions, dim=1)\n",
    "        return predictions\n",
    "\n",
    "# Example usage:\n",
    "# Define the adjacency matrix for spatial processing (A_spatial)\n",
    "# Define the input size, number of layers, and number of heads for the temporal transformer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefddd2c-c2bf-4d89-b345-8db7460bf08e",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f53d5c-0a75-4ecf-80db-72b5cfe0e1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_epoch(epoch, optimizer, loss_function, model, train_loader):\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    for batch_idx, (data,label) in enumerate(train_loader):\n",
    "        #if batch_idx % 200== 0:\n",
    "        #    print(str(batch_idx) + \"/\" + str(len(train_loader)), \" \", total_loss)\n",
    "            \n",
    "        #    break\n",
    "\n",
    "        \n",
    "        label = reshape_to_batches(label, data.batch)\n",
    "        data = data.cuda()\n",
    "        label = label.cuda().float()\n",
    "                \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predictions = model(data)\n",
    "                \n",
    "        loss_value = loss_function(predictions,label)\n",
    "        loss_value.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss_value.item()\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "def validate_epoch(epoch, loss, model, val_loader):\n",
    "    total_loss = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, label) in enumerate(val_loader):\n",
    "            #if batch_idx % 200 == 0:\n",
    "            #    print(str(batch_idx) + \"/\" + str(len(val_loader)), \" \", total_loss)\n",
    "         \n",
    "            label = reshape_to_batches(label, data.batch)\n",
    "            data = data.cuda()\n",
    "            label = label.cuda().float()\n",
    "            \n",
    "            predictions = model(data)\n",
    "            \n",
    "            loss_value = loss(predictions, label)\n",
    "            total_loss += loss_value.item()\n",
    "    return total_loss / len(val_loader)\n",
    "\n",
    "def a_proper_training(num_epoch, model, optimizer, loss_function, train_loader, val_loader, scheduler):\n",
    "    best_epoch = None\n",
    "    best_model = None\n",
    "    best_loss = None\n",
    "    train_losses = list()\n",
    "    val_losses = list()\n",
    "    print(\"Begin Training\")\n",
    "\n",
    "    for epoch in range(num_epoch):\n",
    "        start_time = time.time()  # Start time\n",
    "        train_loss = train_epoch(epoch, optimizer, loss_function, model, train_loader)\n",
    "        val_loss = validate_epoch(0, criterion, model, val_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        scheduler.step(val_loss)  # Update the learning rate based on the validation loss\n",
    "\n",
    "        if epoch == 0 or val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model = copy.deepcopy(model)\n",
    "            best_epoch = epoch\n",
    "\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}/{num_epoch}: Train Loss = {train_loss} Val Loss = {val_loss} Elapsed_time = {elapsed_time // 60}mins\")\n",
    "\n",
    "    return (best_model, best_epoch, train_losses, val_losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba20a1b-2948-4ae5-bed7-7355d61bc0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    model = STGCN(in_channels=1, gcn_layers=1, hidden_channels=8, lstm_layers=1, out_channels=1).cuda()\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=5, verbose=True)\n",
    "\n",
    "    # Now pass the scheduler to the training function\n",
    "    best_model, best_epoch, train_losses, val_losses = a_proper_training(\n",
    "        30, model, optimizer, criterion, train_loader, val_loader, scheduler\n",
    "    )\n",
    "\n",
    "    torch.save(best_model.state_dict(), \"best_ST-GCN_model_direct-connect.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478e2f2b-d8ce-4df2-98af-9ce85954917b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label=\"train\")\n",
    "plt.plot(val_losses, label=\"val\")\n",
    "plt.title(\"MSE Loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c22180-4666-48bf-b0ff-7ee3101d5527",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import copy\n",
    "import time\n",
    "\n",
    "\n",
    "NUM_EPOCHS = 250\n",
    "\n",
    "def objective(trial):\n",
    "    print(\"\\033[41m-------------------------------------------------------------------------------------\\033[0m\")\n",
    "    try:\n",
    "        # Suggest hyperparameters with even values\n",
    "        hidden_channels = trial.suggest_int('hidden_channels', 4, 20, step=2)\n",
    "        gcn_layers = trial.suggest_int('gcn_layers', 1, 4)\n",
    "        lstm_layers = trial.suggest_int('lstm_layers', 1, 10)\n",
    "        learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-1, log=True) \n",
    "\n",
    "        model = STGCN(in_channels=1, gcn_layers=gcn_layers, hidden_channels=hidden_channels, lstm_layers=lstm_layers, out_channels=1).cuda()\n",
    "\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        criterion = nn.MSELoss()\n",
    "        scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=5, verbose=True)\n",
    "\n",
    "        best_loss = float('inf')\n",
    "        patience = 10  # Number of epochs to wait for improvement before stopping\n",
    "        patience_counter = 0  # Counter for epochs without improvement        \n",
    "        best_model = None\n",
    "        train_losses = list()\n",
    "        val_losses = list()\n",
    "        \n",
    "        for epoch in range(NUM_EPOCHS):\n",
    "            train_loss = train_epoch(epoch, optimizer, criterion, model, train_loader)\n",
    "            val_loss = validate_epoch(epoch, criterion, model, val_loader)\n",
    "            train_losses.append(train_loss)\n",
    "            val_losses.append(val_loss)\n",
    "\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "            if val_loss < best_loss:\n",
    "                best_loss = val_loss\n",
    "                best_model = copy.deepcopy(model)\n",
    "                patience_counter = 0  # Reset counter if improvement is observed\n",
    "            else:\n",
    "                patience_counter += 1  # Increment counter if no improvement\n",
    "\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"\\033[34mStopping early at epoch {epoch} due to no improvement in validation loss.\\033[0m\")\n",
    "                break  # Exit the loop if the model hasn't improved for 'patience' epochs\n",
    "        \n",
    "        plt.plot(train_losses, label=\"train\")\n",
    "        plt.plot(val_losses, label=\"val\")\n",
    "        plt.title(\"MSE Loss, lr=\" + str(learning_rate))\n",
    "        plt.legend()\n",
    "        plt.savefig(f'models/direct_connect_lstm/model_{hidden_channels}{gcn_layers}{lstm_layers}_{str(best_loss)[2:8]}.png')\n",
    "        torch.save(best_model.state_dict(), f'models/direct_connect_lstm/model_{hidden_channels}{gcn_layers}{lstm_layers}_{str(best_loss)[2:8]}.pth')\n",
    "\n",
    "        print()\n",
    "        return best_loss\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return float('inf')\n",
    "\n",
    "# Optimize hyperparameters\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=100)  # Define the number of trials\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\" Value: \", trial.value)\n",
    "print(\" Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f85904e-3fec-4f13-9983-cb435b07a2d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
