{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57594bf-9c73-4291-b818-9e1ad179aee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "# Get the directory containing the notebook\n",
    "notebook_dir = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "\n",
    "# Add the directory containing the notebook to sys.path\n",
    "sys.path.append(notebook_dir)\n",
    "\n",
    "# Add the parent directory (which contains the 'dataloaders' directory) to sys.path\n",
    "parent_dir = os.path.abspath(os.path.join(notebook_dir, '.'))\n",
    "sys.path.append(parent_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bce56d-8cda-4a98-805b-b629516a6f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions.loader import getLoader\n",
    "from functions.display_things import *\n",
    "from functions.trainFuncs import *\n",
    "from functions.STGCN import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188e381e",
   "metadata": {},
   "outputs": [],
   "source": [
    "station = \"varberg\"\n",
    "future_steps = 36\n",
    "seq_len = 576\n",
    "batch_size = 64\n",
    "random_seed = 42\n",
    "\n",
    "epochs = 60\n",
    "warmup_steps = int(epochs * 0.2)\n",
    "learning_rate = 0.01\n",
    "hyperN_start=0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb8792b-1c1e-4f2c-9705-a1b1ae361d44",
   "metadata": {},
   "source": [
    "# Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d92c905-ae36-4074-8492-1adac8389c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader = getLoader(station=station, future_steps=future_steps,\n",
    "                                                  seq_len=seq_len, batch_size=batch_size,\n",
    "                                                  random_seed=random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20367ea8-15e9-4025-b27a-84fd7a3de352",
   "metadata": {},
   "source": [
    "# Model Making"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a18916-815d-468a-bf0c-b2a7871f5de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "edge_index_single = torch.tensor([[i, j] for i in range(5) for j in range(5) if i != j], dtype=torch.long).t()\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_channels=1, gcn_hidden_channels=8, gcn_layers=1):\n",
    "        super(GCN, self).__init__()\n",
    "        self.in_conv = GCNConv(in_channels, gcn_hidden_channels)\n",
    "        self.hidden_convs = nn.ModuleList(\n",
    "            [GCNConv(gcn_hidden_channels, gcn_hidden_channels) for _ in range(gcn_layers - 1)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, weights, batch):        \n",
    "        x = x.float()\n",
    "        \n",
    "        x = self.in_conv(x, edge_index)\n",
    "        for i, conv in enumerate(self.hidden_convs[:-1]):\n",
    "            x = F.relu(x)\n",
    "            x = conv(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        if weights:\n",
    "            x = reshape_to_batches(x, batch)\n",
    "            print(x.shape)  # [64, 5, 576, 4]\n",
    "            batch_size, num_stations, seq_len, feature_size = x.shape\n",
    "            gcn_last_layer_weight, gcn_last_layer_bias = weights[0]\n",
    "\n",
    "            # Reshape to apply linear transformation\n",
    "            x = x.view(batch_size * num_stations, seq_len, feature_size)\n",
    "            x = x.view(-1, feature_size)\n",
    "            x = F.linear(x, gcn_last_layer_weight, gcn_last_layer_bias)\n",
    "            x = x.view(batch_size * num_stations, seq_len, -1)\n",
    "            x = x.view(batch_size, num_stations, seq_len, -1)\n",
    "            x = reshape_from_batches(x)\n",
    "        return x\n",
    "class SimpleTransformer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layer_size, output_size, nhead, seq_length, num_layers=1, dropout=0.1):\n",
    "        super(SimpleTransformer, self).__init__()\n",
    "\n",
    "        self.seq_length = seq_length\n",
    "        self.output_size = output_size\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        \n",
    "        self.embeddingIn = nn.Linear(input_size, hidden_layer_size)\n",
    "        self.embeddingTGT = nn.Linear(output_size, hidden_layer_size)\n",
    "        \n",
    "        self.PositionalEncoding = PositionalEncoding(max_len=1000, d_model=hidden_layer_size)\n",
    "        \n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(d_model=hidden_layer_size, nhead=nhead, \n",
    "                                       dim_feedforward=4*hidden_layer_size, dropout=dropout, \n",
    "                                       activation='gelu')\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.decoder_layers = nn.ModuleList([\n",
    "            nn.TransformerDecoderLayer(d_model=hidden_layer_size, nhead=nhead,\n",
    "                                       dim_feedforward=4*hidden_layer_size, dropout=dropout, \n",
    "                                       activation='gelu')\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.linear1 = nn.Linear(hidden_layer_size, output_size)\n",
    "                \n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n",
    "        return mask\n",
    "        \n",
    "    def forward(self, x, tgt=None, last_value=None, inference=False, weights=None):\n",
    "        last_value = torch.unsqueeze(last_value, dim=2)\n",
    "\n",
    "        initial_tgt = last_value\n",
    "        \n",
    "        tgt_input = torch.cat([last_value, tgt[:, :-1]], dim=1)\n",
    "        \n",
    "        x = self.embeddingIn(x)\n",
    "        x = self.PositionalEncoding(x)\n",
    "        enc_mask = self.generate_square_subsequent_mask(x.size(1)).to(tgt.device)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        \n",
    "        for layer in self.encoder_layers[:-1]:\n",
    "            x = layer(x, src_mask=enc_mask)\n",
    "\n",
    "        if weights:\n",
    "            batch_size, num_stations, seq_len, feature_size = x.shape\n",
    "            transformer_last_layer_weight, transformer_last_layer_bias = weights[1]\n",
    "\n",
    "            # Reshape to apply linear transformation\n",
    "            x = x.view(batch_size * num_stations, seq_len, feature_size)\n",
    "            x = x.view(-1, feature_size)\n",
    "            x = F.linear(x, transformer_last_layer_weight, transformer_last_layer_bias)\n",
    "            x = x.view(batch_size * num_stations, seq_len, -1)\n",
    "            x = x.view(batch_size, num_stations, seq_len, -1)\n",
    "            x = x.permute(1, 0, 2)\n",
    "        else:\n",
    "            x = self.encoder_layers[-1](x, src_mask=enc_mask)\n",
    "        \n",
    "        encoder_output = x.permute(1, 0, 2)\n",
    "                \n",
    "        if inference:\n",
    "            tgt_gen = initial_tgt\n",
    "            generated_sequence = torch.zeros((initial_tgt.size(0), self.seq_length, self.output_size), device=x.device)\n",
    "            encoder_output = encoder_output.permute(1, 0, 2)\n",
    "\n",
    "            for i in range(self.seq_length):\n",
    "                tgt_emb = self.embeddingTGT(tgt_gen)\n",
    "                tgt_emb = self.PositionalEncoding(tgt_emb)\n",
    "                tgt_emb = tgt_emb.permute(1, 0, 2)\n",
    "\n",
    "                for layer in self.decoder_layers[:-1]:\n",
    "                    decoder_output = layer(tgt_emb, encoder_output)\n",
    "                \n",
    "                output_step = self.linear1(decoder_output[-1, :, :])\n",
    "                output_step = output_step.unsqueeze(1) \n",
    "\n",
    "                generated_sequence[:, i:i+1, :] = output_step\n",
    "\n",
    "                tgt_gen = torch.cat((tgt_gen, output_step), dim=1)\n",
    "\n",
    "                if tgt_gen.size(1) > self.seq_length:\n",
    "                    tgt_gen = tgt_gen[:, 1:, :]\n",
    "\n",
    "            return generated_sequence\n",
    "\n",
    "        else:\n",
    "            tgt = self.embeddingTGT(tgt_input)\n",
    "            tgt = self.PositionalEncoding(tgt)\n",
    "            tgt = tgt.permute(1, 0, 2)\n",
    "\n",
    "            tgt_mask = self.generate_square_subsequent_mask(tgt.size(0)).to(tgt.device)\n",
    "\n",
    "            encoder_output = encoder_output.permute(1, 0, 2)\n",
    "            \n",
    "            for layer in self.decoder_layers[:-1]:\n",
    "                decoder_output = layer(tgt, encoder_output, tgt_mask=tgt_mask)\n",
    "            \n",
    "            output = self.linear1(decoder_output)\n",
    "\n",
    "            return output.permute(1, 0, 2)\n",
    "\n",
    "\n",
    "\n",
    "class STGCN(nn.Module):\n",
    "    def __init__(self, in_channels, gcn_layers, hidden_channels, transformer_hidden_size, transformer_num_layers, transformer_nhead, out_channels):\n",
    "        super(STGCN, self).__init__()\n",
    "        self.GCN = GCN(in_channels=in_channels, gcn_hidden_channels=hidden_channels, gcn_layers=gcn_layers)\n",
    "\n",
    "        self.transformer = SimpleTransformer(\n",
    "            input_size=hidden_channels, hidden_layer_size=transformer_hidden_size,\n",
    "            output_size=out_channels, seq_length=36, num_layers=transformer_num_layers,\n",
    "            nhead=transformer_nhead\n",
    "        ).cuda()\n",
    "\n",
    "    def forward(self, data, inference=False, weights=None):\n",
    "        batch = data.batch\n",
    "        label = data.y\n",
    "        label = torch.squeeze(label, 2)\n",
    "\n",
    "        data.x = data.x.float()\n",
    "        data.edge_attr = data.edge_attr.float()\n",
    "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
    "\n",
    "        x = self.GCN(x, edge_index, edge_attr, weights=weights, batch=batch)\n",
    "        x = reshape_to_batches(x, batch)\n",
    "        last_value = reshape_to_batches(data.x[:, -1, :], batch)\n",
    "        label = reshape_to_batches(label, batch)\n",
    "\n",
    "        predictions = []\n",
    "\n",
    "        for station_data, station_label, station_last_value in zip(x.permute(1, 0, 2, 3), label.permute(1, 0, 2, 3), last_value.permute(1, 0, 2)):\n",
    "            output = self.transformer(station_data, station_label, station_last_value, inference, weights=weights)\n",
    "            predictions.append(output)\n",
    "\n",
    "        predictions = torch.stack(predictions, dim=1)\n",
    "        return predictions\n",
    "\n",
    "\n",
    "class WeightGenerator(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(WeightGenerator, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_channels, hidden_channels)\n",
    "        self.fc2 = nn.Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class FusionNetwork(nn.Module):\n",
    "    def __init__(self, stgcn_params, weight_gen_params):\n",
    "        super(FusionNetwork, self).__init__()\n",
    "        self.stgcn = STGCN(**stgcn_params).cuda()\n",
    "        self.weight_generator = WeightGenerator(**weight_gen_params).cuda()\n",
    "        self.hyper_on_state = False\n",
    "\n",
    "    def hyper_on(self, state):\n",
    "        self.hyper_on_state = state\n",
    "\n",
    "    def freeze_stgcn(self):\n",
    "        for param in self.stgcn.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def unfreeze_stgcn(self):\n",
    "        for param in self.stgcn.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    def forward(self, data, inference=False):\n",
    "        if self.hyper_on_state:\n",
    "            x_flat = data.x.view(data.x.size(0), -1)\n",
    "            generated_weights = self.weight_generator(x_flat)\n",
    "\n",
    "            gcn_last_layer_weight = generated_weights[:, :self.stgcn.GCN.hidden_convs[-1].lin.weight.numel()]\n",
    "            gcn_last_layer_weight = gcn_last_layer_weight.reshape(\n",
    "                (gcn_last_layer_weight.shape[0] // 5, 5, 4, 4)\n",
    "            )\n",
    "            gcn_last_layer_weight = gcn_last_layer_weight.sum(dim=1)\n",
    "\n",
    "            transformer_last_layer_weight = generated_weights[:, self.stgcn.GCN.hidden_convs[-1].lin.weight.numel():]\n",
    "            transformer_last_layer_weight = transformer_last_layer_weight.reshape(\n",
    "                (transformer_last_layer_weight.shape[0] // 5, 5, 36, 12)\n",
    "            )\n",
    "            transformer_last_layer_weight = transformer_last_layer_weight.sum(dim=1)\n",
    "\n",
    "            # Ensure the generated weights have requires_grad=True\n",
    "            gcn_last_layer_weight = gcn_last_layer_weight.detach().clone().requires_grad_(True)\n",
    "            transformer_last_layer_weight = transformer_last_layer_weight.detach().clone().requires_grad_(True)\n",
    "\n",
    "            gcn_last_layer_bias = torch.zeros(gcn_last_layer_weight.size(0), device=gcn_last_layer_weight.device).requires_grad_(True)\n",
    "            transformer_last_layer_bias = torch.zeros(transformer_last_layer_weight.size(0), device=transformer_last_layer_weight.device).requires_grad_(True)\n",
    "\n",
    "            weights = [(gcn_last_layer_weight, gcn_last_layer_bias), (transformer_last_layer_weight, transformer_last_layer_bias)]\n",
    "\n",
    "            predictions = self.stgcn(data, inference, weights=weights)\n",
    "        else:\n",
    "            predictions = self.stgcn(data, inference)\n",
    "\n",
    "        return predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefddd2c-c2bf-4d89-b345-8db7460bf08e",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba20a1b-2948-4ae5-bed7-7355d61bc0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage:\n",
    "stgcn_params = {\n",
    "    'in_channels': 1,\n",
    "    'gcn_layers': 3,\n",
    "    'hidden_channels': 4,\n",
    "    'transformer_hidden_size': 12,\n",
    "    'transformer_num_layers': 2,\n",
    "    'transformer_nhead': 2,\n",
    "    'out_channels': 1\n",
    "}\n",
    "\n",
    "weight_gen_params = {\n",
    "    'in_channels': 1 * 576,  # Adjust based on input dimensions\n",
    "    'hidden_channels': 64,\n",
    "    'out_channels': 4 * 4 + 2 * 12 * 12 + 150 - 6 # Adjust based on the weight dimensions\n",
    "}\n",
    "\n",
    "model = FusionNetwork(stgcn_params, weight_gen_params).cuda()\n",
    "\n",
    "model.load_state_dict(torch.load('HyperNetwork_pretrained_on_varnamo.pth'))\n",
    "\n",
    "model.freeze_stgcn()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a219b99-6209-49a0-9067-1d96ce4e94b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.MSELoss()    \n",
    "\n",
    "# Define the lambda function for scheduling with Noam-style learning rate decay\n",
    "def lr_lambda(current_step: int, d_model: int, warmup_steps: int) -> float:\n",
    "    current_step+=1\n",
    "    return (d_model ** (-0.5)) * min((current_step ** (-0.5)), current_step * (warmup_steps ** (-1.5)))\n",
    "\n",
    "d_model = stgcn_params['transformer_hidden_size']\n",
    "scheduler = LambdaLR(optimizer, lr_lambda=lambda step: lr_lambda(step, d_model, warmup_steps))    \n",
    "\n",
    "# Now pass the scheduler to the training function\n",
    "best_model, best_epoch, train_losses, val_losses, lrs = a_proper_training(\n",
    "    epochs, model, optimizer, criterion, train_loader, val_loader, scheduler, hyperN_start=hyperN_start\n",
    ")\n",
    "\n",
    "torch.save(best_model.state_dict(), \"HyperNetwork_pretrained_on_varnamo-finetuned_on_varberg.pth\")\n",
    "\n",
    "plt.plot(train_losses, label=\"train\")\n",
    "plt.plot(val_losses, label=\"val\")\n",
    "#plt.plot(lrs, label=\"learning rates\")\n",
    "\n",
    "plt.title(\"MSE Loss\")\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3890429-75fe-4f56-9ab0-59d00560f9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.eval()\n",
    "\n",
    "predictAndDisplay(station, val_loader, best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708bf258-49ff-4483-9070-8fa51a4fd189",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
