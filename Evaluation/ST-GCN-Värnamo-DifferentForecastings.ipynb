{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57594bf-9c73-4291-b818-9e1ad179aee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install networkx\n",
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bce56d-8cda-4a98-805b-b629516a6f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import copy\n",
    "import random\n",
    "\n",
    "import networkx as nx\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Batch, Data, Dataset\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import time\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188e381e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "!python --version\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available.\")\n",
    "else:\n",
    "    print(\"No GPU detected.\")\n",
    "\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb8792b-1c1e-4f2c-9705-a1b1ae361d44",
   "metadata": {},
   "source": [
    "# Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d92c905-ae36-4074-8492-1adac8389c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\"donken\",\n",
    "         \"Holmgrens\",\n",
    "         \"IONITY\",\n",
    "         \"Jureskogs_Vattenfall\",\n",
    "         \"UFC\"]\n",
    "\n",
    "nr_of_data_points = 163618\n",
    "splits=[0.8, 0.9]\n",
    "\n",
    "data_dict = {}\n",
    "\n",
    "\n",
    "for f in files:\n",
    "    data = pd.read_csv('data_' + f + '_5T_k-10.csv')\n",
    "    #data.info()\n",
    "    data.set_index('Unnamed: 0', inplace=True)\n",
    "    \n",
    "    data = data.drop(columns=data.columns.difference(['Occupancy']))\n",
    "    \n",
    "    data_dict[f] = data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20367ea8-15e9-4025-b27a-84fd7a3de352",
   "metadata": {},
   "source": [
    "# Model Making"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e160da-671d-4715-9a63-0537a8e7d6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add nodes\n",
    "num_nodes = 5\n",
    "G.add_nodes_from(range(num_nodes))\n",
    "\n",
    "Jureskogs_Vattenfall = 0\n",
    "IONITY = 1\n",
    "donken = 2\n",
    "Holmgrens = 3\n",
    "UFC = 4\n",
    "\n",
    "# Define edges to connect specific nodes with custom weights\n",
    "edges_to_connect = [\n",
    "    (Jureskogs_Vattenfall, IONITY, 230),\n",
    "    (Jureskogs_Vattenfall, UFC, 750),\n",
    "    (Jureskogs_Vattenfall, Holmgrens, 750),\n",
    "    (Jureskogs_Vattenfall, donken, 650),\n",
    "    (IONITY, UFC, 550),\n",
    "    (IONITY, Holmgrens, 500),\n",
    "    (IONITY, donken, 450),\n",
    "    (UFC, Holmgrens, 280),\n",
    "    (UFC, donken, 550),\n",
    "    (Holmgrens, donken, 550)\n",
    "]\n",
    "\n",
    "# Add edges with custom weights\n",
    "for edge in edges_to_connect:\n",
    "    G.add_edge(edge[0], edge[1], weight=edge[2])\n",
    "\n",
    "# Create adjacency matrix with weights\n",
    "adj_matrix = nx.adjacency_matrix(G).todense()\n",
    "\n",
    "torch_adj_matrix = torch.Tensor(adj_matrix)\n",
    "\n",
    "edge_index = torch_adj_matrix.nonzero(as_tuple=False).t().contiguous()\n",
    "edge_attr = torch_adj_matrix[torch_adj_matrix.nonzero()].reshape(-1, 1)\n",
    "\n",
    "\n",
    "# Define colors for nodes\n",
    "node_colors = ['Yellow'] * num_nodes\n",
    "\n",
    "print(G.nodes)\n",
    "\n",
    "# Draw the graph\n",
    "pos = nx.spring_layout(G)  # positions for all nodes\n",
    "nx.draw(G, pos, with_labels=True, node_color=node_colors, node_size=700, font_size=10)\n",
    "edge_labels = nx.get_edge_attributes(G, 'weight')\n",
    "nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels)\n",
    "\n",
    "# Show the graph\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ac3e2e-f9b7-4995-8f25-a26eb7ea7642",
   "metadata": {},
   "outputs": [],
   "source": [
    "four_hours = 48\n",
    "three_hours = 36\n",
    "two_hours = 24\n",
    "one_hour = 12\n",
    "half_hour = 6\n",
    "\n",
    "future_steps = four_hours\n",
    "seq_len = 576\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "indices = list(range(len(data) - future_steps - seq_len))\n",
    "random.shuffle(indices)\n",
    "print(len(indices))\n",
    "\n",
    "train_i = indices[:int(len(indices)*0.8)] \n",
    "val_i = indices[int(len(indices)*0.8):int(len(indices)*0.9)]\n",
    "test_i = indices[int(len(indices)*0.9):]\n",
    "\n",
    "class datasetMaker(Dataset):\n",
    "    def __init__(self, station_data, indices_conversion, edge_index, edge_attr, seq_len, future_steps, batch_size):\n",
    "        self.station_data = station_data\n",
    "        self.indices_conversion = indices_conversion\n",
    "        self.size = station_data[\"donken\"].shape[0]\n",
    "        self.edge_index = edge_index\n",
    "        self.edge_attr = edge_attr\n",
    "        self.seq_len = seq_len\n",
    "        self.future_steps = future_steps\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices_conversion) - self.seq_len - self.future_steps\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        index = self.indices_conversion[index]\n",
    "        \n",
    "        seq_end = index + self.seq_len\n",
    "        fut_end = index + self.seq_len + self.future_steps\n",
    "        \n",
    "        node_features = []\n",
    "        for i, (station, data) in enumerate(self.station_data.items()):\n",
    "            node_feature = data.iloc[index:seq_end].values\n",
    "            node_features.append(node_feature)\n",
    "        node_features = torch.tensor(np.array(node_features)).float()\n",
    "\n",
    "        labels = []\n",
    "        for i, (station, data) in enumerate(self.station_data.items()):\n",
    "            label = data.iloc[seq_end:fut_end].values\n",
    "            labels.append(label)\n",
    "        labels = torch.unsqueeze(torch.tensor(np.array(labels)), dim=2).float()\n",
    "        \n",
    "        Gdata = Data(x=node_features, y=labels, edge_index=self.edge_index, edge_attr=self.edge_attr)\n",
    "\n",
    "        return Gdata, labels\n",
    "\n",
    "    \n",
    "def custom_collate(batch):\n",
    "    label = torch.cat([i[1] for i in batch])\n",
    "    \n",
    "    label = label.squeeze(3)\n",
    "    \n",
    "    batch = Batch.from_data_list([b[0] for b in batch])\n",
    "\n",
    "    \n",
    "    return batch, label\n",
    "\n",
    "train_dataset = datasetMaker(data_dict, train_i, edge_index, edge_attr, seq_len, future_steps, batch_size)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, drop_last=True, collate_fn=custom_collate)\n",
    "\n",
    "val_dataset = datasetMaker(data_dict, val_i, edge_index, edge_attr, seq_len, future_steps, batch_size)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=True, drop_last=True, collate_fn=custom_collate)\n",
    "\n",
    "test_dataset = datasetMaker(data_dict, test_i, edge_index, edge_attr, seq_len, future_steps, batch_size)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True, drop_last=True, collate_fn=custom_collate)\n",
    "\n",
    "\n",
    "print(\"train len \", len(train_loader))\n",
    "print(\"val len   \", len(val_loader))\n",
    "print(\"test len  \", len(test_loader))\n",
    "\n",
    "for data, label in train_loader:\n",
    "    print(data)\n",
    "    print(label.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a18916-815d-468a-bf0c-b2a7871f5de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def reshape_to_batches(x, batch_description):\n",
    "    \"\"\"\n",
    "        Does something like this:\n",
    "        torch.Size([28, 576, 64]) --> torch.Size([4, 7, 576, 64])\n",
    "    \"\"\"\n",
    "    num_splits = batch_description.max().item() + 1\n",
    "    new_shape_dim_0 = num_splits\n",
    "    new_shape_dim_1 = x.size(0) // new_shape_dim_0\n",
    "    new_shape = torch.Size([new_shape_dim_0, new_shape_dim_1] + list(x.size()[1:]))\n",
    "    reshaped_tensor = x.view(new_shape)\n",
    "    return reshaped_tensor\n",
    "\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels=1, gcn_hidden_channels=8, gcn_layers=1):\n",
    "        super(GCN, self).__init__()\n",
    "        self.in_conv = GCNConv(in_channels, gcn_hidden_channels)\n",
    "        self.hidden_convs = [GCNConv(gcn_hidden_channels, gcn_hidden_channels).cuda() for i in range(gcn_layers - 1)]\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = x.float()\n",
    "        x = self.in_conv(x, edge_index)\n",
    "        for conv in self.hidden_convs:\n",
    "            x = F.relu(x)\n",
    "            x = conv(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        return x\n",
    "\n",
    "class SimpleTransformer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layer_size, output_size, nhead, seq_length, num_layers=1, dropout=0.1):\n",
    "        super(SimpleTransformer, self).__init__()\n",
    "\n",
    "        self.seq_length = seq_length\n",
    "        self.output_size = output_size\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        \n",
    "        self.embeddingIn = nn.Linear(input_size, hidden_layer_size)\n",
    "        self.embeddingTGT = nn.Linear(output_size, hidden_layer_size)\n",
    "        \n",
    "        self.PositionalEncoding = PositionalEncoding(max_len=1000, d_model=hidden_layer_size)\n",
    "        \n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model=hidden_layer_size, nhead=nhead, \n",
    "                                                    dim_feedforward=4*hidden_layer_size, dropout=dropout, \n",
    "                                                    activation='gelu')\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer=encoder_layers, num_layers=num_layers)\n",
    "        # tr\n",
    "        decoder_layers = nn.TransformerDecoderLayer(d_model=hidden_layer_size, nhead=nhead,\n",
    "                                                    dim_feedforward=4*hidden_layer_size, dropout=dropout, \n",
    "                                                    activation='gelu')\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer=decoder_layers, num_layers=num_layers)\n",
    "\n",
    "        self.linear1 = nn.Linear(hidden_layer_size, output_size)\n",
    "                \n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n",
    "        return mask\n",
    "        \n",
    "    def forward(self, x, tgt=None, last_value=None, inference=False):\n",
    "        last_value = torch.unsqueeze(last_value, dim=2)\n",
    "\n",
    "        initial_tgt = last_value#x[:, -1:]\n",
    "        #start_value = x[:, -1:]\n",
    "        \n",
    "        tgt_input = torch.cat([last_value, tgt[:, :-1]], dim=1)\n",
    "        \n",
    "        x = self.embeddingIn(x)\n",
    "        x = self.PositionalEncoding(x)\n",
    "        enc_mask = self.generate_square_subsequent_mask(x.size(1)).to(tgt.device)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        encoder_output = self.transformer_encoder(x, mask=enc_mask)\n",
    "        encoder_output = encoder_output.permute(1, 0, 2)\n",
    "        \n",
    "        if inference:\n",
    "            tgt_gen = initial_tgt\n",
    "            #print(encoder_output.shape)\n",
    "            #encoder_output = encoder_output.permute(1, 0, 2)\n",
    "            #print(encoder_output.shape)\n",
    "            #print(tgt_gen.shape)\n",
    "            generated_sequence = torch.zeros((initial_tgt.size(0), self.seq_length, self.output_size), device=x.device)\n",
    "            encoder_output = encoder_output.permute(1,0,2)\n",
    "\n",
    "            for i in range(self.seq_length):\n",
    "                #print(tgt_gen.shape)\n",
    "                \n",
    "                tgt_emb = self.embeddingTGT(tgt_gen)\n",
    "                #print(tgt_emb.shape)\n",
    "                \n",
    "                tgt_emb = self.PositionalEncoding(tgt_emb)\n",
    "                tgt_emb = tgt_emb.permute(1, 0, 2)\n",
    "                #print(tgt_emb.shape)\n",
    "\n",
    "                decoder_output = self.transformer_decoder(tgt_emb, encoder_output)\n",
    "\n",
    "                output_step = self.linear1(decoder_output[-1, :, :])\n",
    "                output_step = output_step.unsqueeze(1) \n",
    "\n",
    "                generated_sequence[:, i:i+1, :] = output_step\n",
    "\n",
    "                tgt_gen = torch.cat((tgt_gen, output_step), dim=1)\n",
    "                #start_value = torch.unsqueeze(x[:, -1:, 1], 1)\n",
    "\n",
    "                if tgt_gen.size(1) > self.seq_length:\n",
    "                    tgt_gen = tgt_gen[:, 1:, :]\n",
    "\n",
    "            return generated_sequence\n",
    "\n",
    "        else:\n",
    "            tgt = self.embeddingTGT(tgt_input)\n",
    "            tgt = self.PositionalEncoding(tgt)\n",
    "            tgt = tgt.permute(1, 0, 2)\n",
    "\n",
    "            tgt_mask = self.generate_square_subsequent_mask(tgt.size(0)).to(tgt.device)\n",
    "\n",
    "            encoder_output = encoder_output.permute(1,0,2)\n",
    "            \n",
    "            decoder_output = self.transformer_decoder(tgt, encoder_output, tgt_mask=tgt_mask)\n",
    "            #try dropout here\n",
    "            output = self.linear1(decoder_output)\n",
    "\n",
    "            return output.permute(1, 0, 2)\n",
    "        \n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # Correct the shaping of pe to [1, max_len, d_model]\n",
    "        pe = pe.unsqueeze(0)\n",
    "        \n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #print(x.shape)\n",
    "        #print(self.pe[:, :x.size(1), :].shape)\n",
    "        # Adjust slicing of pe to match the sequence length of x\n",
    "        # pe is broadcasted correctly across the batch dimension\n",
    "        return x + self.pe[:, :x.size(1), :]\n",
    "\n",
    "class STGCN(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, gcn_layers, hidden_channels, transformer_hidden_size, transformer_num_layers, transformer_nhead, out_channels):\n",
    "        super(STGCN, self).__init__()\n",
    "        print(\"\\033[100mhidden_channels:\", hidden_channels,\n",
    "              \"   GCN hidden layers:\", gcn_layers,\n",
    "              \"   transformer_hidden_size:\", transformer_hidden_size,\n",
    "              \"   transformer_num_layers:\", transformer_num_layers,\n",
    "              \"   transformer_nhead:\", transformer_nhead, \"\\033[0m\")\n",
    "\n",
    "        self.GCN = GCN(in_channels=in_channels, gcn_hidden_channels=hidden_channels, gcn_layers=gcn_layers)\n",
    "\n",
    "        self.transformer = SimpleTransformer(input_size = hidden_channels, hidden_layer_size=transformer_hidden_size,\n",
    "                                             output_size=out_channels, seq_length=48, num_layers=transformer_num_layers,\n",
    "                                             nhead=transformer_nhead).cuda()\n",
    "        \n",
    "    def forward(self, data, inference=False):    \n",
    "        batch = data.batch\n",
    "        label = data.y\n",
    "        label = torch.squeeze(label, 2)\n",
    "        \n",
    "        data.x = data.x.float()  # Convert node features to Double\n",
    "        data.edge_attr = data.edge_attr.float()  # Convert edge attributes to Double\n",
    "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
    "       \n",
    "        # Spatial processing\n",
    "        x = self.GCN(x, edge_index, edge_attr)\n",
    "\n",
    "        x = reshape_to_batches(x, batch)\n",
    "        last_value = reshape_to_batches(data.x[:,-1,:],batch)\n",
    "        label = reshape_to_batches(label, batch)\n",
    "                \n",
    "        # Reshape and pass data through the model for each station\n",
    "        predictions = []\n",
    "       \n",
    "        for station_data, station_label, station_last_value in zip(x.permute(1,0,2,3), label.permute(1,0,2,3), last_value.permute(1,0,2)):\n",
    "            output = self.transformer(station_data, station_label, station_last_value, inference)\n",
    "            predictions.append(output)\n",
    "\n",
    "        # Concatenate predictions for all stations\n",
    "        predictions = torch.stack(predictions, dim=1)\n",
    "        return predictions\n",
    "\n",
    "# Example usage:\n",
    "# Define the adjacency matrix for spatial processing (A_spatial)\n",
    "# Define the input size, number of layers, and number of heads for the temporal transformer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefddd2c-c2bf-4d89-b345-8db7460bf08e",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f53d5c-0a75-4ecf-80db-72b5cfe0e1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_epoch(epoch, optimizer, loss_function, model, train_loader):\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    for batch_idx, (data,label) in enumerate(train_loader):\n",
    "\n",
    "        label = reshape_to_batches(label, data.batch)\n",
    "        data = data.cuda()\n",
    "        label = label.cuda().float()\n",
    "                \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predictions = model(data, inference=False)\n",
    "                \n",
    "        loss_value = loss_function(predictions,label)\n",
    "        loss_value.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss_value.item()\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "def validate_epoch(epoch, loss, model, val_loader):\n",
    "    total_loss = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, label) in enumerate(val_loader):\n",
    "\n",
    "            label = reshape_to_batches(label, data.batch)\n",
    "            data = data.cuda()\n",
    "            label = label.cuda().float()\n",
    "            \n",
    "            predictions = model(data, inference=True)\n",
    "            \n",
    "            loss_value = loss(predictions, label)\n",
    "            total_loss += loss_value.item()\n",
    "    return total_loss / len(val_loader)\n",
    "import time\n",
    "import copy\n",
    "\n",
    "def a_proper_training(num_epoch, model, optimizer, loss_function, train_loader, val_loader, scheduler, patience=200):\n",
    "    best_epoch = None\n",
    "    best_model = None\n",
    "    best_loss = float('inf')\n",
    "    train_losses = list()\n",
    "    val_losses = list()\n",
    "    lrs = list()\n",
    "\n",
    "    # Early stopping variables\n",
    "    patience_counter = 0  # to count the number of epochs without improvement\n",
    "    stop_training = False\n",
    "\n",
    "    print(\"Begin Training\")\n",
    "\n",
    "    for epoch in range(num_epoch):\n",
    "        if stop_training:\n",
    "            break\n",
    "\n",
    "        start_time = time.time()  # Start time\n",
    "        train_loss = train_epoch(epoch, optimizer, loss_function, model, train_loader)\n",
    "        val_loss = validate_epoch(epoch, loss_function, model, val_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        scheduler.step()\n",
    "        lrs.append(optimizer.param_groups[0]['lr'])\n",
    "        \n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model = copy.deepcopy(model)\n",
    "            best_epoch = epoch\n",
    "            patience_counter = 0  # reset counter if there's an improvement\n",
    "        else:\n",
    "            patience_counter += 1  # increment counter if no improvement\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Stopping early at epoch {epoch + 1}\")\n",
    "            stop_training = True\n",
    "\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        print(f\"Epoch {epoch + 1}/{num_epoch}: Train Loss = {train_loss} Val Loss = {val_loss} Elapsed_time = {elapsed_time // 60}mins\")\n",
    "\n",
    "    return (best_model, best_epoch, train_losses, val_losses, lrs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba20a1b-2948-4ae5-bed7-7355d61bc0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "\n",
    "    epochs = 100\n",
    "    model = STGCN(in_channels=1, gcn_layers=2, hidden_channels=2, transformer_hidden_size=12,\n",
    "                  transformer_num_layers=1, transformer_nhead=3, out_channels=1).cuda()\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Define the lambda function for scheduling with Noam-style learning rate decay\n",
    "    def lr_lambda(current_step: int, d_model: int, warmup_steps: int) -> float:\n",
    "        current_step+=1\n",
    "        return (d_model ** (-0.5)) * min((current_step ** (-0.5)), current_step * (warmup_steps ** (-1.5)))\n",
    "\n",
    "    warmup_steps = int(epochs * 0.1)\n",
    "    d_model = 10\n",
    "    scheduler = LambdaLR(optimizer, lr_lambda=lambda step: lr_lambda(step, d_model, warmup_steps))    \n",
    "\n",
    "    # Now pass the scheduler to the training function\n",
    "    best_model, best_epoch, train_losses, val_losses, lrs = a_proper_training(\n",
    "        epochs, model, optimizer, criterion, train_loader, val_loader, scheduler\n",
    "    )\n",
    "\n",
    "    torch.save(best_model.state_dict(), \"best_ST-GCN_Transformer_4hour.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478e2f2b-d8ce-4df2-98af-9ce85954917b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(train_losses, label=\"train\")\n",
    "plt.plot(val_losses, label=\"val\")\n",
    "#plt.plot(lrs, label=\"learning rates\")\n",
    "\n",
    "plt.title(\"MSE Loss\")\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a219b99-6209-49a0-9067-1d96ce4e94b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import copy\n",
    "import time\n",
    "\n",
    "\n",
    "NUM_EPOCHS = 250\n",
    "\n",
    "def objective(trial):\n",
    "    print(\"\\033[41m-------------------------------------------------------------------------------------\\033[0m\")\n",
    "    try:\n",
    "        # Suggest hyperparameters with even values\n",
    "        hidden_channels = trial.suggest_int('hidden_channels', 2, 14, step=2)\n",
    "        gcn_layers = trial.suggest_int('gcn_layers', 1, 4)\n",
    "        transformer_num_layers = trial.suggest_int('transformer_num_layers', 1, 6)\n",
    "        transformer_nhead = trial.suggest_int('transformer_nhead', 1, 6)\n",
    "        factor = trial.suggest_int('factor', 2, 12, step=2)\n",
    "        transformer_hidden_size = transformer_nhead * factor\n",
    "        learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-1, log=True) \n",
    "\n",
    "\n",
    "        model = STGCN(in_channels=1,\n",
    "                      gcn_layers=gcn_layers,\n",
    "                      hidden_channels=hidden_channels, \n",
    "                      transformer_hidden_size=transformer_hidden_size, \n",
    "                      transformer_num_layers=transformer_num_layers,\n",
    "                      transformer_nhead=transformer_nhead,\n",
    "                      out_channels=1).cuda()\n",
    "\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        criterion = nn.MSELoss()\n",
    "\n",
    "        # Define the lambda function for scheduling with Noam-style learning rate decay\n",
    "        def lr_lambda(current_step: int, d_model: int, warmup_steps: int) -> float:\n",
    "            current_step+=1\n",
    "            return (d_model ** (-0.5)) * min((current_step ** (-0.5)), current_step * (warmup_steps ** (-1.5)))\n",
    "\n",
    "        warmup_steps = NUM_EPOCHS // 3#int(NUM_EPOCHS * 0.3)\n",
    "        d_model = transformer_hidden_size\n",
    "        scheduler = LambdaLR(optimizer, lr_lambda=lambda step: lr_lambda(step, d_model, warmup_steps))    \n",
    "\n",
    "        best_loss = float('inf')\n",
    "        patience = 20  # Number of epochs to wait for improvement before stopping\n",
    "        patience_counter = 0  # Counter for epochs without improvement\n",
    "        best_model = None\n",
    "        train_losses = list()\n",
    "        val_losses = list()\n",
    "        \n",
    "        for epoch in range(NUM_EPOCHS):\n",
    "            train_loss = train_epoch(epoch, optimizer, criterion, model, train_loader)\n",
    "            val_loss = validate_epoch(epoch, criterion, model, val_loader)\n",
    "            train_losses.append(train_loss)\n",
    "            val_losses.append(val_loss)\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "            if val_loss < best_loss:\n",
    "                best_loss = val_loss\n",
    "                best_model = copy.deepcopy(model)\n",
    "                patience_counter = 0  # Reset counter if improvement is observed\n",
    "            else:\n",
    "                patience_counter += 1  # Increment counter if no improvement\n",
    "\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"\\033[34mStopping early at epoch {epoch} due to no improvement in validation loss.\\033[0m\")\n",
    "                break  # Exit the loop if the model hasn't improved for 'patience' epochs\n",
    "        \n",
    "        plt.plot(train_losses, label=\"train\")\n",
    "        plt.plot(val_losses, label=\"val\")\n",
    "        plt.title(\"MSE Loss, lr=\" + str(learning_rate))\n",
    "        plt.legend()\n",
    "        plt.savefig(f'models/direct_connect_transformer/model_{hidden_channels}{gcn_layers}{transformer_num_layers}{transformer_nhead}{transformer_hidden_size}_{str(best_loss)[2:8]}.png')\n",
    "        torch.save(best_model.state_dict(), f'models/direct_connect_transformer/model_{hidden_channels}{gcn_layers}{transformer_num_layers}{transformer_nhead}{transformer_hidden_size}_{str(best_loss)[2:8]}.pth')\n",
    "                \n",
    "        print()\n",
    "        return best_loss\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print()\n",
    "        return float('inf')\n",
    "\n",
    "# Optimize hyperparameters\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=100)  # Define the number of trials\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\" Value: \", trial.value)\n",
    "print(\" Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3890429-75fe-4f56-9ab0-59d00560f9b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
