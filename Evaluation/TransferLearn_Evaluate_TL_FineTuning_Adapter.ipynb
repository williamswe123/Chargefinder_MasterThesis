{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2648ddb2-ec3c-4477-bb28-2ebc8287c932",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "# Get the directory containing the notebook\n",
    "notebook_dir = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "\n",
    "# Add the directory containing the notebook to sys.path\n",
    "sys.path.append(notebook_dir)\n",
    "\n",
    "# Add the parent directory (which contains the 'dataloaders' directory) to sys.path\n",
    "parent_dir = os.path.abspath(os.path.join(notebook_dir, '.'))\n",
    "sys.path.append(parent_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117296d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions.loader import getLoader\n",
    "from functions.display_things import *\n",
    "from functions.trainFuncs import a_proper_training\n",
    "from functions.STGCN import STGCN\n",
    "from TransferLearn_Evaluate_TL_FineTuning_Adapter import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac9bc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "subsamples = [0.05, 0.2, 0.5, 1]\n",
    "pretrain_seeds = [42, 43, 44]\n",
    "finetune_seeds = [47, 48, 51]\n",
    "\n",
    "\n",
    "for subsample in subsamples:\n",
    "\n",
    "    mse = []\n",
    "    for pretrain_seed in pretrain_seeds:\n",
    "        print()\n",
    "        print(\"\\tpretrained_seed\", \":\", pretrain_seed)\n",
    "        for finetune_seed in finetune_seeds:\n",
    "            best_model, train_losses, val_losses, test_losses, best_epoch = do_da_test(pretrain_station=\"varnamo\",\n",
    "                                                                                       finetune_station=\"varberg\",\n",
    "                                                                                       pretrain_seed=pretrain_seed,\n",
    "                                                                                       finetune_seed=finetune_seed,\n",
    "                                                                                       epochs=30,\n",
    "                                                                                       subsample=subsample,\n",
    "                                                                                       verbose=False)\n",
    "            print(\"\\t\\tfinetune seed,\", finetune_seed, \":\", test_losses[best_epoch])\n",
    "            mse.append(test_losses[best_epoch])\n",
    "            \n",
    "    avg = sum(mse) / len(mse)\n",
    "    std_div = np.std(mse)\n",
    "    \n",
    "    print(\"for subsample\", subsample, \", average:\" , avg, \", std div:\", std_div)\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcefb34",
   "metadata": {},
   "source": [
    "# Loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e65ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adapter(nn.Module):\n",
    "    def __init__(self, input_size, hidden_network):\n",
    "        super(Adapter, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, input_size//2)\n",
    "        self.fc2 = nn.Linear(input_size//2, input_size)\n",
    "        self.hidden_network = hidden_network\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, data, inference):\n",
    "        x = data.x\n",
    "        x = reshape_to_batches(x, data.batch)\n",
    "        batch_size, stations, seq_len, features = x.shape\n",
    "        \n",
    "        x = x.view(batch_size, -1)        \n",
    "        # Apply the fully connected layer\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        # Reshape back to original shape\n",
    "        x = x.view(64, 5, 576, 1)        \n",
    "        \n",
    "        x = reshape_from_batches(x)\n",
    "        data.x = x\n",
    "        \n",
    "        x = self.hidden_network(data, inference)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a40151",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('Transfer Learning/trained_on_varnamo.pth'))\n",
    "\n",
    "adapter_network = Adapter(2880, model).cuda()\n",
    "\n",
    "for param in adapter_network.hidden_network.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in adapter_network.fc1.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in adapter_network.fc2.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "count_parameters(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22960af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.MSELoss()    \n",
    "\n",
    "# Define the lambda function for scheduling with Noam-style learning rate decay\n",
    "def lr_lambda(current_step: int, d_model: int, warmup_steps: int) -> float:\n",
    "    current_step+=1\n",
    "    return (d_model ** (-0.5)) * min((current_step ** (-0.5)), current_step * (warmup_steps ** (-1.5)))\n",
    "\n",
    "d_model = transformer_hidden_size\n",
    "scheduler = LambdaLR(optimizer, lr_lambda=lambda step: lr_lambda(step, d_model, warmup_steps))    \n",
    "\n",
    "best_model, best_epoch, train_losses, val_losses, lrs = a_proper_training(\n",
    "    epochs, adapter_network, optimizer, criterion, train_loader, val_loader, scheduler\n",
    ")\n",
    "\n",
    "torch.save(best_model.state_dict(), \"trained_on_varnamo-finetuned_on_varberg_Adapter.pth\")\n",
    "\n",
    "plt.plot(train_losses, label=\"train\")\n",
    "plt.plot(val_losses, label=\"val\")\n",
    "#plt.plot(lrs, label=\"learning rates\")\n",
    "\n",
    "plt.title(\"MSE Loss\")\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12ebecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_model.eval()\n",
    "\n",
    "predictAndDisplay(station, test_loader, best_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
