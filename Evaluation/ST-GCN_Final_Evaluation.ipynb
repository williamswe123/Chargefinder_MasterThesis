{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731cfec7-dbc2-478d-8b18-933cbe652bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import copy\n",
    "\n",
    "import networkx as nx\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Batch, Data, Dataset\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import time\n",
    "\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8613b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "!python --version\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available.\")\n",
    "else:\n",
    "    print(\"No GPU detected.\")\n",
    "\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1047763-19d7-4dbf-864f-c6e71c59a5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\"donken\",\n",
    "         \"Holmgrens\",\n",
    "         \"IONITY\",\n",
    "         \"Jureskogs_Vattenfall\",\n",
    "         \"UFC\"]\n",
    "\n",
    "nr_of_data_points = 163618\n",
    "splits=[0.8, 0.9]\n",
    "\n",
    "train_data = {}\n",
    "val_data = {}\n",
    "test_data = {}\n",
    "\n",
    "for f in files:\n",
    "    data = pd.read_csv('data_' + f + '_5T_k-10.csv')\n",
    "    #data.info()\n",
    "    data.set_index('Unnamed: 0', inplace=True)\n",
    "    \n",
    "    data = data.drop(columns=data.columns.difference(['Occupancy']))\n",
    "    \n",
    "    train_data[f] = data[:int(splits[0]*nr_of_data_points)]\n",
    "    val_data[f] = data[int(splits[0]*nr_of_data_points):int(splits[1]*nr_of_data_points)]\n",
    "    test_data[f] = data[int(splits[1]*nr_of_data_points):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641c4692-90ff-4d3d-a832-b6910b074bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add nodes\n",
    "num_nodes = 5\n",
    "G.add_nodes_from(range(num_nodes))\n",
    "\n",
    "Jureskogs_Vattenfall = 0\n",
    "IONITY = 1\n",
    "donken = 2\n",
    "Holmgrens = 3\n",
    "UFC = 4\n",
    "\n",
    "# Define edges to connect specific nodes with custom weights\n",
    "edges_to_connect = [\n",
    "    (Jureskogs_Vattenfall, IONITY, 230),\n",
    "    (Jureskogs_Vattenfall, UFC, 750),\n",
    "    (Jureskogs_Vattenfall, Holmgrens, 750),\n",
    "    (Jureskogs_Vattenfall, donken, 650),\n",
    "    (IONITY, UFC, 550),\n",
    "    (IONITY, Holmgrens, 500),\n",
    "    (IONITY, donken, 450),\n",
    "    (UFC, Holmgrens, 280),\n",
    "    (UFC, donken, 550),\n",
    "    (Holmgrens, donken, 550)\n",
    "]\n",
    "\n",
    "# Add edges with custom weights\n",
    "for edge in edges_to_connect:\n",
    "    G.add_edge(edge[0], edge[1], weight=edge[2])\n",
    "\n",
    "# Create adjacency matrix with weights\n",
    "adj_matrix = nx.adjacency_matrix(G).todense()\n",
    "\n",
    "torch_adj_matrix = torch.Tensor(adj_matrix)\n",
    "\n",
    "edge_index = torch_adj_matrix.nonzero(as_tuple=False).t().contiguous()\n",
    "edge_attr = torch_adj_matrix[torch_adj_matrix.nonzero()].reshape(-1, 1)\n",
    "\n",
    "\n",
    "# Define colors for nodes\n",
    "node_colors = ['Yellow'] * num_nodes\n",
    "\n",
    "print(G.nodes)\n",
    "\n",
    "# Draw the graph\n",
    "pos = nx.spring_layout(G)  # positions for all nodes\n",
    "nx.draw(G, pos, with_labels=True, node_color=node_colors, node_size=700, font_size=10)\n",
    "edge_labels = nx.get_edge_attributes(G, 'weight')\n",
    "nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels)\n",
    "\n",
    "# Show the graph\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667ecc66-ff91-49a5-800e-cbd8402963e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class datasetMaker(Dataset):\n",
    "    def __init__(self, station_data, edge_index, edge_attr, seq_len, future_steps, batch_size):\n",
    "        self.station_data = station_data\n",
    "        self.size = station_data[\"donken\"].shape[0]\n",
    "        self.edge_index = edge_index\n",
    "        self.edge_attr = edge_attr\n",
    "        self.seq_len = seq_len\n",
    "        self.future_steps = future_steps\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size - self.seq_len - self.future_steps\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        seq_end = index + self.seq_len\n",
    "        fut_end = index + self.seq_len + self.future_steps\n",
    "        \n",
    "        node_features = []\n",
    "        for i, (station, data) in enumerate(self.station_data.items()):\n",
    "            node_feature = data.iloc[index:seq_end].values\n",
    "            node_features.append(node_feature)\n",
    "        node_features = torch.tensor(np.array(node_features)).float()\n",
    "\n",
    "        labels = []\n",
    "        for i, (station, data) in enumerate(self.station_data.items()):\n",
    "            label = data.iloc[seq_end:fut_end].values\n",
    "            labels.append(label)\n",
    "        labels = torch.unsqueeze(torch.tensor(np.array(labels)), dim=2)\n",
    "        \n",
    "        Gdata = Data(x=node_features, y=labels, edge_index=self.edge_index, edge_attr=self.edge_attr)\n",
    "\n",
    "        return Gdata, labels\n",
    "\n",
    "    \n",
    "def custom_collate(batch):\n",
    "    label = torch.cat([i[1] for i in batch])\n",
    "    \n",
    "    label = label.squeeze(3)\n",
    "    \n",
    "    batch = Batch.from_data_list([b[0] for b in batch])\n",
    "\n",
    "    \n",
    "    return batch, label\n",
    "\n",
    "\n",
    "future_steps = 36\n",
    "seq_len = 576\n",
    "batch_size = 64\n",
    "\n",
    "train_dataset = datasetMaker(train_data, edge_index, edge_attr, seq_len, future_steps, batch_size)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, drop_last=True, collate_fn=custom_collate)\n",
    "\n",
    "val_dataset = datasetMaker(val_data, edge_index, edge_attr, seq_len, future_steps, batch_size)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=True, drop_last=True, collate_fn=custom_collate)\n",
    "\n",
    "test_dataset = datasetMaker(test_data, edge_index, edge_attr, seq_len, future_steps, batch_size)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True, drop_last=True, collate_fn=custom_collate)\n",
    "\n",
    "\n",
    "print(\"train len \", len(train_loader))\n",
    "print(\"val len   \", len(val_loader))\n",
    "print(\"test len  \", len(test_loader))\n",
    "\n",
    "\n",
    "for data, label in train_loader:\n",
    "    print(data)\n",
    "    print(label.shape)\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5f68aa-869f-4265-a988-c8f8e397fcd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def reshape_to_batches(x, batch_description):\n",
    "    \"\"\"\n",
    "        Does something like this:\n",
    "        torch.Size([28, 576, 64]) --> torch.Size([4, 7, 576, 64])\n",
    "    \"\"\"\n",
    "    num_splits = batch_description.max().item() + 1\n",
    "    new_shape_dim_0 = num_splits\n",
    "    new_shape_dim_1 = x.size(0) // new_shape_dim_0\n",
    "    new_shape = torch.Size([new_shape_dim_0, new_shape_dim_1] + list(x.size()[1:]))\n",
    "    reshaped_tensor = x.view(new_shape)\n",
    "    return reshaped_tensor\n",
    "\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels=1, gcn_hidden_channels=8, gcn_layers=1):\n",
    "        super(GCN, self).__init__()\n",
    "        self.in_conv = GCNConv(in_channels, gcn_hidden_channels)\n",
    "        self.hidden_convs = [GCNConv(gcn_hidden_channels, gcn_hidden_channels).cuda() for i in range(gcn_layers - 1)]\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = x.float()\n",
    "        x = self.in_conv(x, edge_index)\n",
    "        for conv in self.hidden_convs:\n",
    "            x = F.relu(x)\n",
    "            x = conv(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        return x\n",
    "\n",
    "class SimpleTransformer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layer_size, output_size, nhead, seq_length, num_layers=1, dropout=0.1):\n",
    "        super(SimpleTransformer, self).__init__()\n",
    "\n",
    "        self.seq_length = seq_length\n",
    "        self.output_size = output_size\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        \n",
    "        self.embeddingIn = nn.Linear(input_size, hidden_layer_size)\n",
    "        self.embeddingTGT = nn.Linear(output_size, hidden_layer_size)\n",
    "        \n",
    "        self.PositionalEncoding = PositionalEncoding(max_len=1000, d_model=hidden_layer_size)\n",
    "        \n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model=hidden_layer_size, nhead=nhead, \n",
    "                                                    dim_feedforward=4*hidden_layer_size, dropout=dropout, \n",
    "                                                    activation='gelu')\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer=encoder_layers, num_layers=num_layers)\n",
    "        # tr\n",
    "        decoder_layers = nn.TransformerDecoderLayer(d_model=hidden_layer_size, nhead=nhead,\n",
    "                                                    dim_feedforward=4*hidden_layer_size, dropout=dropout, \n",
    "                                                    activation='gelu')\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer=decoder_layers, num_layers=num_layers)\n",
    "\n",
    "        self.linear1 = nn.Linear(hidden_layer_size, output_size)\n",
    "                \n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n",
    "        return mask\n",
    "        \n",
    "    def forward(self, x, tgt=None, last_value=None, inference=False):\n",
    "        last_value = torch.unsqueeze(last_value, dim=2)\n",
    "\n",
    "        initial_tgt = last_value#x[:, -1:]\n",
    "        #start_value = x[:, -1:]\n",
    "        \n",
    "        tgt_input = torch.cat([last_value, tgt[:, :-1]], dim=1)\n",
    "        \n",
    "        x = self.embeddingIn(x)\n",
    "        x = self.PositionalEncoding(x)\n",
    "        enc_mask = self.generate_square_subsequent_mask(x.size(1)).to(tgt.device)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        encoder_output = self.transformer_encoder(x, mask=enc_mask)\n",
    "        encoder_output = encoder_output.permute(1, 0, 2)\n",
    "        \n",
    "        if inference:\n",
    "            tgt_gen = initial_tgt\n",
    "            #print(encoder_output.shape)\n",
    "            #encoder_output = encoder_output.permute(1, 0, 2)\n",
    "            #print(encoder_output.shape)\n",
    "            #print(tgt_gen.shape)\n",
    "            generated_sequence = torch.zeros((initial_tgt.size(0), self.seq_length, self.output_size), device=x.device)\n",
    "            encoder_output = encoder_output.permute(1,0,2)\n",
    "\n",
    "            for i in range(self.seq_length):\n",
    "                #print(tgt_gen.shape)\n",
    "                \n",
    "                tgt_emb = self.embeddingTGT(tgt_gen)\n",
    "                #print(tgt_emb.shape)\n",
    "                \n",
    "                tgt_emb = self.PositionalEncoding(tgt_emb)\n",
    "                tgt_emb = tgt_emb.permute(1, 0, 2)\n",
    "                #print(tgt_emb.shape)\n",
    "\n",
    "                decoder_output = self.transformer_decoder(tgt_emb, encoder_output)\n",
    "\n",
    "                output_step = self.linear1(decoder_output[-1, :, :])\n",
    "                output_step = output_step.unsqueeze(1) \n",
    "\n",
    "                generated_sequence[:, i:i+1, :] = output_step\n",
    "\n",
    "                tgt_gen = torch.cat((tgt_gen, output_step), dim=1)\n",
    "                #start_value = torch.unsqueeze(x[:, -1:, 1], 1)\n",
    "\n",
    "                if tgt_gen.size(1) > self.seq_length:\n",
    "                    tgt_gen = tgt_gen[:, 1:, :]\n",
    "\n",
    "            return generated_sequence\n",
    "\n",
    "        else:\n",
    "            tgt = self.embeddingTGT(tgt_input)\n",
    "            tgt = self.PositionalEncoding(tgt)\n",
    "            tgt = tgt.permute(1, 0, 2)\n",
    "\n",
    "            tgt_mask = self.generate_square_subsequent_mask(tgt.size(0)).to(tgt.device)\n",
    "\n",
    "            encoder_output = encoder_output.permute(1,0,2)\n",
    "            \n",
    "            decoder_output = self.transformer_decoder(tgt, encoder_output, tgt_mask=tgt_mask)\n",
    "            #try dropout here\n",
    "            output = self.linear1(decoder_output)\n",
    "\n",
    "            return output.permute(1, 0, 2)\n",
    "        \n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # Correct the shaping of pe to [1, max_len, d_model]\n",
    "        pe = pe.unsqueeze(0)\n",
    "        \n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #print(x.shape)\n",
    "        #print(self.pe[:, :x.size(1), :].shape)\n",
    "        # Adjust slicing of pe to match the sequence length of x\n",
    "        # pe is broadcasted correctly across the batch dimension\n",
    "        return x + self.pe[:, :x.size(1), :]\n",
    "\n",
    "class STGCN(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, gcn_layers, hidden_channels, transformer_hidden_size, transformer_num_layers, transformer_nhead, out_channels):\n",
    "        super(STGCN, self).__init__()\n",
    "        print(\"\\033[100mhidden_channels:\", hidden_channels,\n",
    "              \"   GCN hidden layers:\", gcn_layers,\n",
    "              \"   transformer_hidden_size:\", transformer_hidden_size,\n",
    "              \"   transformer_num_layers:\", transformer_num_layers,\n",
    "              \"   transformer_nhead:\", transformer_nhead, \"\\033[0m\")\n",
    "\n",
    "        self.GCN = GCN(in_channels=in_channels, gcn_hidden_channels=hidden_channels, gcn_layers=gcn_layers)\n",
    "\n",
    "        self.transformer = SimpleTransformer(input_size = hidden_channels, hidden_layer_size=transformer_hidden_size,\n",
    "                                             output_size=out_channels, seq_length=36, num_layers=transformer_num_layers,\n",
    "                                             nhead=transformer_nhead).cuda()\n",
    "        \n",
    "    def forward(self, data, inference=False):    \n",
    "        batch = data.batch\n",
    "        label = data.y\n",
    "        label = torch.squeeze(label, 2)\n",
    "        \n",
    "        data.x = data.x.float()  # Convert node features to Double\n",
    "        data.edge_attr = data.edge_attr.float()  # Convert edge attributes to Double\n",
    "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
    "       \n",
    "        # Spatial processing\n",
    "        x = self.GCN(x, edge_index, edge_attr)\n",
    "\n",
    "        x = reshape_to_batches(x, batch)\n",
    "        last_value = reshape_to_batches(data.x[:,-1,:],batch)\n",
    "        label = reshape_to_batches(label, batch)\n",
    "                \n",
    "        # Reshape and pass data through the model for each station\n",
    "        predictions = []\n",
    "       \n",
    "        for station_data, station_label, station_last_value in zip(x.permute(1,0,2,3), label.permute(1,0,2,3), last_value.permute(1,0,2)):\n",
    "            output = self.transformer(station_data, station_label, station_last_value, inference)\n",
    "            predictions.append(output)\n",
    "\n",
    "        # Concatenate predictions for all stations\n",
    "        predictions = torch.stack(predictions, dim=1)\n",
    "        return predictions\n",
    "\n",
    "# Example usage:\n",
    "# Define the adjacency matrix for spatial processing (A_spatial)\n",
    "# Define the input size, number of layers, and number of heads for the temporal transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481224c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiStepLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
    "        super(MultiStepLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "        # Flatten LSTM parameters\n",
    "        self.lstm.flatten_parameters()\n",
    "        \n",
    "        # Fully connected layer to map LSTM output to desired output_size\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x, future=1):\n",
    "        \n",
    "        predictions = []\n",
    "        \n",
    "        \n",
    "        for _ in range(future):\n",
    "            # Initialize hidden state and cell state        \n",
    "            batch_size, sequence_length, _ = x.size()\n",
    "            h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)\n",
    "            c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)\n",
    "\n",
    "            # LSTM forward pass\n",
    "            out, (h0, c0) = self.lstm(x, (h0, c0))\n",
    "            \n",
    "            pred = out[:, -1, :]\n",
    "            \n",
    "            x = torch.cat((x, pred.unsqueeze(1)), dim=1)\n",
    "            \n",
    "            t = self.fc(pred)\n",
    "            predictions.append(t) # Append occupancy to predictions\n",
    "            \n",
    "\n",
    "        # Stack predictions along the sequence length dimension'\n",
    "        predictions = torch.cat(predictions, dim=1)\n",
    "        \n",
    "        predictions = torch.unsqueeze(predictions, dim = 2)\n",
    "        return predictions\n",
    "    \n",
    "class STGCNLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, gcn_layers, hidden_channels, lstm_layers, out_channels):\n",
    "        super(STGCN, self).__init__()\n",
    "        \n",
    "        print(\"\\033[100mhidden_channels:\", hidden_channels,\n",
    "              \"   GCN hidden layers:\", gcn_layers,\n",
    "              \"   lstm_layers:\", lstm_layers, \"\\033[0m\")\n",
    "\n",
    "        \n",
    "        self.GCN = GCN(in_channels=in_channels, gcn_hidden_channels=hidden_channels, gcn_layers=gcn_layers)\n",
    "\n",
    "        self.lstm = MultiStepLSTM(hidden_channels, hidden_channels, out_channels, lstm_layers)\n",
    "        \n",
    "    def forward(self, data):    \n",
    " \n",
    "        batch = data.batch\n",
    "        \n",
    "        data.x = data.x.float()  # Convert node features to Double\n",
    "        data.edge_attr = data.edge_attr.float()  # Convert edge attributes to Double\n",
    "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
    "       \n",
    "        # Spatial processing\n",
    "        x = self.GCN(x, edge_index, edge_attr)\n",
    "\n",
    "        x = reshape_to_batches(x, batch)\n",
    "        # Reshape and pass data through the model for each station\n",
    "        predictions = []\n",
    "        for station_data in x.permute(1,0,2,3):  # Iterate over each station\n",
    "            #station_data = station_data.permute(1, 0, 2)  # Reshape for LSTM (batch_first=True)\n",
    "            output = self.lstm(station_data, future=future_steps)\n",
    "            predictions.append(output)\n",
    "\n",
    "        # Concatenate predictions for all stations\n",
    "        predictions = torch.stack(predictions, dim=1)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2221dae",
   "metadata": {},
   "source": [
    "# Loading model ST-GCN LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314daf05",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = STGCNLSTM\n",
    "\n",
    "# Load the state dictionary\n",
    "best_model.load_state_dict(torch.load('best_ST-GCN_model_direct-connect-transformer_final.pth'))\n",
    "\n",
    "# Set the model to evaluation mode if you are testing (not training)\n",
    "best_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b74162-4137-4740-9820-8db48b603e9e",
   "metadata": {},
   "source": [
    "# Loading model ST-GCN transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab8ec62-efd1-4081-8d94-64786f378253",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = STGCN(in_channels=1, gcn_layers=2, hidden_channels=2, transformer_hidden_size=12,\n",
    "                  transformer_num_layers=1, transformer_nhead=3, out_channels=1).cuda()\n",
    "\n",
    "# Load the state dictionary\n",
    "best_model.load_state_dict(torch.load('best_ST-GCN_model_direct-connect-transformer_final.pth'))\n",
    "\n",
    "# Set the model to evaluation mode if you are testing (not training)\n",
    "best_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81384a46",
   "metadata": {},
   "source": [
    "# Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a310850-abb0-4c78-99ae-7a0215f4f2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings to keep the output clean\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Assuming val_loader is your DataLoader and best_model is your model\n",
    "\n",
    "# Initialize storage for predictions and true values for all models\n",
    "all_predictions = []  # ST-GCN predictions\n",
    "all_labels = []  # ST-GCN labels\n",
    "all_arima_forecasts = []  # MA(40) ARIMA predictions\n",
    "all_ar_forecasts = []  # AR(60) predictions\n",
    "all_arima_36_forecasts = []  # ARIMA(36, 1, 36) predictions\n",
    "all_last_value_forecasts = []  # Last known value predictions\n",
    "\n",
    "batch_count = 0  # Number of batches to process\n",
    "sequence_count = 0  # Track the total number of processed sequences\n",
    "\n",
    "for data, label in test_loader:\n",
    "    \n",
    "    label = reshape_to_batches(label, data.batch)\n",
    "    label = label.float().cpu()\n",
    "    data = data.cuda()\n",
    "\n",
    "    # Get predictions from the ST-GCN model\n",
    "    predictions = best_model(data, inference=True)\n",
    "    \n",
    "    # Convert tensors to numpy arrays\n",
    "    predictions = predictions.detach().cpu().numpy()\n",
    "    label = label.numpy()\n",
    "    \n",
    "    # Iterate over each sequence in the batch\n",
    "    for i in range(label.shape[0]):\n",
    "        if sequence_count >= 16:  # Stop after 16 predictions\n",
    "            break\n",
    "\n",
    "        actual_data = label[i, 2, :, 0]  # Actual data used for all models\n",
    "        all_predictions.append(predictions[i, 2, :, 0])  # ST-GCN predictions\n",
    "        all_labels.append(actual_data)  # True labels\n",
    "\n",
    "        # MA(40) ARIMA model\n",
    "        ma_model = ARIMA(actual_data, order=(0, 0, 40))\n",
    "        ma_model_fitted = ma_model.fit()\n",
    "        ma_forecast = ma_model_fitted.forecast(steps=len(actual_data))\n",
    "        all_arima_forecasts.append(ma_forecast)\n",
    "        \n",
    "        # AR(60) model\n",
    "        ar_model = ARIMA(actual_data, order=(60, 0, 0))\n",
    "        ar_model_fitted = ar_model.fit()\n",
    "        ar_forecast = ar_model_fitted.forecast(steps=len(actual_data))\n",
    "        all_ar_forecasts.append(ar_forecast)\n",
    "\n",
    "        # ARIMA(36, 1, 36) model\n",
    "        arima_36_model = ARIMA(actual_data, order=(36, 1, 36))\n",
    "        arima_36_model_fitted = arima_36_model.fit()\n",
    "        arima_36_forecast = arima_36_model_fitted.forecast(steps=len(actual_data))\n",
    "        all_arima_36_forecasts.append(arima_36_forecast)\n",
    "\n",
    "        # Last known value prediction\n",
    "        last_value = actual_data[-1]\n",
    "        last_value_forecast = np.full(len(actual_data), last_value)\n",
    "        all_last_value_forecasts.append(last_value_forecast)\n",
    "        \n",
    "        sequence_count += 1\n",
    "    \n",
    "    batch_count += 1\n",
    "    if sequence_count >= 16:\n",
    "        break\n",
    "\n",
    "# Plotting the predictions\n",
    "fig, axes = plt.subplots(4, 4, figsize=(20, 20))  # 4 rows and 4 columns for 16 sequences\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(16):\n",
    "    axes[i].plot(all_predictions[i], label=\"ST-GCN Predictions\", color=\"blue\")\n",
    "    axes[i].plot(all_arima_forecasts[i], label=\"MA(40) ARIMA Predictions\", color=\"green\")\n",
    "    axes[i].plot(all_ar_forecasts[i], label=\"AR(60) Predictions\", color=\"red\")\n",
    "    axes[i].plot(all_arima_36_forecasts[i], label=\"ARIMA(36, 1, 36) Predictions\", color=\"purple\")\n",
    "    axes[i].plot(all_last_value_forecasts[i], label=\"Last Known Value\", color=\"gray\", linestyle='--')\n",
    "    axes[i].plot(all_labels[i], label=\"True Values\", color=\"orange\")\n",
    "    axes[i].set_title(f\"Prediction {i+1}\")\n",
    "    axes[i].legend()\n",
    "    axes[i].set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fd814b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_squared_log_error, median_absolute_error\n",
    "\n",
    "# Define metrics calculation functions\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "def r_squared(y_true, y_pred):\n",
    "    return r2_score(y_true, y_pred)\n",
    "\n",
    "best_model.eval()\n",
    "\n",
    "# Data containers with explicit tracking\n",
    "metrics_data = {'station': files, 'mse': [], 'rmse': [], 'mae': [], 'r2': [], 'msle': [], 'medae': []}\n",
    "for metric in ['mse', 'rmse', 'mae', 'r2', 'msle', 'medae']:\n",
    "    metrics_data[metric] = [[] for _ in files]  # List of lists for each station\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_data, batch_labels in test_loader:\n",
    "        batch_labels = reshape_to_batches(batch_labels, batch_data.batch)\n",
    "        batch_data = batch_data.cuda()\n",
    "        batch_labels = batch_labels.cuda().float()\n",
    "        batch_predictions = best_model(batch_data, inference=True)\n",
    "\n",
    "        batch_predictions = batch_predictions.view(-1, len(files), 36)  # Assume '36' prediction steps\n",
    "        batch_labels = batch_labels.view(-1, len(files), 36)\n",
    "\n",
    "        for idx, station_name in enumerate(files):\n",
    "            p = batch_predictions[:, idx, :].cpu().numpy()\n",
    "            l = batch_labels[:, idx, :].cpu().numpy()\n",
    "\n",
    "            # Calculate metrics for each station and append\n",
    "            mse = mean_squared_error(l, p)\n",
    "            metrics_data['mse'][idx].append(mse)\n",
    "            metrics_data['rmse'][idx].append(root_mean_squared_error(l, p))\n",
    "            metrics_data['mae'][idx].append(mean_absolute_error(l, p))\n",
    "            metrics_data['r2'][idx].append(r_squared(l, p))\n",
    "            metrics_data['msle'][idx].append(mean_squared_log_error(l, p))\n",
    "            metrics_data['medae'][idx].append(median_absolute_error(l, p))\n",
    "\n",
    "# Average metrics across batches\n",
    "for metric in ['mse', 'rmse', 'mae', 'r2', 'msle', 'medae']:\n",
    "    metrics_data[metric] = [np.mean(vals) for vals in metrics_data[metric]]\n",
    "\n",
    "# Plotting setup\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(files)))\n",
    "\n",
    "for ax, metric in zip(axes.flatten(), ['mse', 'rmse', 'mae', 'r2', 'msle', 'medae']):\n",
    "    ax.bar(metrics_data['station'], metrics_data[metric], color=colors)\n",
    "    ax.set_title(f'{metric.upper()} by Station')\n",
    "    ax.set_xlabel('Station')\n",
    "    ax.set_ylabel(metric.upper())\n",
    "    ax.set_xticks(metrics_data['station'])  # Ensure x-ticks correspond to stations\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147da2fd-9aa3-467f-8949-24034b0b477c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.scatter([12, 24, 36, 48], [0.0035, 0.0070, 0.0105, 0.0140], color='orange', label='AsynST-GCN', marker='*', s=200)\n",
    "plt.scatter([12, 24, 36, 48], [0.0041, 0.0083, 0.0098, 0.0177], color='red', label='ST-GCN (Transformer)', marker='*', s=200)\n",
    "plt.scatter([12, 24, 36, 48], [0.0050, 0.0100, 0.0150, 0.0200], color='blue', label='ST-GCN (LSTM)', marker='*', s=200)\n",
    "plt.scatter([12, 24, 36, 48], [0.0020, 0.0040, 0.0060, 0.0080], color='darkgreen', label='LSTM', marker='*', s=200)\n",
    "plt.scatter([12, 24, 36, 48], [0.0045, 0.0090, 0.0135, 0.0180], color='limegreen', label='Transformer', marker='*', s=200)\n",
    "plt.scatter([12, 24, 36, 48], [0.0060, 0.0120, 0.0180, 0.0240], color='cyan', label='ARIMA', marker='*', s=200)\n",
    "plt.scatter([12, 24, 36, 48], [0.0030, 0.0060, 0.0090, 0.0120], color='purple', label='MA', marker='*', s=200)\n",
    "plt.scatter([12, 24, 36, 48], [0.0055, 0.0110, 0.0165, 0.0220], color='black', label='AR', marker='*', s=200)\n",
    "\n",
    "plt.plot([12, 24, 36, 48], [0.0035, 0.0070, 0.0105, 0.0140], color='orange', linestyle='--')  \n",
    "plt.plot([12, 24, 36, 48], [0.0041, 0.0083, 0.0098, 0.0177], color='red', linestyle='--')  \n",
    "plt.plot([12, 24, 36, 48], [0.0050, 0.0100, 0.0150, 0.0200], color='blue', linestyle='--')  \n",
    "plt.plot([12, 24, 36, 48], [0.0020, 0.0040, 0.0060, 0.0080], color='darkgreen', linestyle='--')  \n",
    "plt.plot([12, 24, 36, 48], [0.0045, 0.0090, 0.0135, 0.0180], color='limegreen', linestyle='--')  \n",
    "plt.plot([12, 24, 36, 48], [0.0060, 0.0120, 0.0180, 0.0240], color='cyan', linestyle='--')  \n",
    "plt.plot([12, 24, 36, 48], [0.0030, 0.0060, 0.0090, 0.0120], color='purple', linestyle='--')  \n",
    "plt.plot([12, 24, 36, 48], [0.0055, 0.0110, 0.0165, 0.0220], color='black', linestyle='--')  \n",
    "\n",
    "plt.text(12, 0.002, '1 hour', ha='center', fontsize=10)\n",
    "plt.text(24, 0.002, '2 hours', ha='center', fontsize=10)\n",
    "plt.text(36, 0.002, '3 hours', ha='center', fontsize=10)\n",
    "plt.text(48, 0.002, '4 hours', ha='center', fontsize=10)\n",
    "\n",
    "plt.title('Forecasting Horizon Comparison')\n",
    "plt.xlabel('Forecasting window (5-minute data points)')\n",
    "plt.ylabel('MSE')\n",
    "\n",
    "plt.xlim(0, 60)\n",
    "plt.ylim(0, 0.02)\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df518a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "all_mse_by_step = {station: [[] for _ in range(36)] for station in files}  # Dictionary to store MSE values for each station and prediction step\n",
    "num_steps = 36  # Total number of prediction steps\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_data, batch_labels in test_loader:\n",
    "        batch_labels = reshape_to_batches(batch_labels, batch_data.batch)\n",
    "        batch_data = batch_data.cuda()\n",
    "        batch_labels = batch_labels.cuda().float()\n",
    "\n",
    "        batch_predictions = best_model(batch_data, inference=True)\n",
    "\n",
    "        for station_index, station_name in enumerate(files):\n",
    "            for step in range(num_steps):\n",
    "                mse_step = F.mse_loss(batch_predictions[:, station_index, step], batch_labels[:, station_index, step]).item()\n",
    "                all_mse_by_step[station_name][step].append(mse_step)\n",
    "\n",
    "mean_mse_by_step = {station: [np.mean(step_mse_values) for step_mse_values in all_mse_by_step[station]] for station in files}\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "for station in files:\n",
    "    plt.plot(range(1, num_steps + 1), mean_mse_by_step[station], marker='o', label=station)\n",
    "\n",
    "plt.title('Mean MSE for Each Prediction Step Across Sequences by Station')\n",
    "plt.xlabel('Prediction Step')\n",
    "plt.ylabel('Mean MSE')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316caf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "best_model.eval()\n",
    "\n",
    "# Data containers for each station\n",
    "station_data = {name: {'predictions': [], 'labels': []} for name in files}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_data, batch_labels in test_loader:\n",
    "        # Assuming your reshape_to_batches handles batch data appropriately\n",
    "        batch_labels = reshape_to_batches(batch_labels, batch_data.batch)\n",
    "        batch_data = batch_data.cuda()\n",
    "        batch_labels = batch_labels.cuda().float()\n",
    "\n",
    "        # Obtain predictions\n",
    "        batch_predictions = best_model(batch_data, inference=True)\n",
    "        \n",
    "        batch_predictions = batch_predictions.view(-1, len(files), 36)  # Assume '36' prediction steps\n",
    "        batch_labels = batch_labels.view(-1, len(files), 36)\n",
    "\n",
    "        for idx, station_name in enumerate(files):\n",
    "            # Extract predictions and actuals for one station\n",
    "            predictions = batch_predictions[:, idx, :].flatten().cpu().numpy()\n",
    "            labels = batch_labels[:, idx, :].flatten().cpu().numpy()\n",
    "\n",
    "            # Store data for plotting\n",
    "            station_data[station_name]['predictions'].extend(predictions)\n",
    "            station_data[station_name]['labels'].extend(labels)\n",
    "\n",
    "# Plot predicted vs actual values for each station\n",
    "fig, axs = plt.subplots(nrows=1, ncols=5, figsize=(15, 10))  # Adjust subplot layout to 1 row\n",
    "for ax, station_name in zip(axs, files):\n",
    "    preds = np.array(station_data[station_name]['predictions'])\n",
    "    actuals = np.array(station_data[station_name]['labels'])\n",
    "\n",
    "    lims = [0, 1]\n",
    "    \n",
    "    ax.scatter(preds, actuals, alpha=0.1)\n",
    "    ax.plot(lims, lims, 'r--', lw=2)  # Change reference line to red\n",
    "    ax.set_aspect('equal', adjustable='box')  # Ensure that axes are equally scaled\n",
    "    ax.set_xlim(lims)\n",
    "    ax.set_ylim(lims)\n",
    "    ax.set_title(f'Predicted vs Actual for {station_name}')\n",
    "    ax.set_xlabel('Predicted Values')\n",
    "    ax.set_ylabel('Actual Values')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee02771",
   "metadata": {},
   "source": [
    "# Random predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b118c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data containers for each station\n",
    "station_data = {name: {'predictions': [], 'labels': []} for name in files}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_data, batch_labels in test_loader:\n",
    "        # Assuming your reshape_to_batches handles batch data appropriately\n",
    "        batch_labels = reshape_to_batches(batch_labels, batch_data.batch)\n",
    "        batch_data = batch_data.cuda()\n",
    "        batch_labels = batch_labels.cuda().float()\n",
    "\n",
    "        # Generate random predictions that match the shape of batch_labels\n",
    "        batch_predictions = np.random.rand(*batch_labels.size()).astype(np.float32)  # Ensure it's float32 to match typical PyTorch dtype\n",
    "\n",
    "        for idx, station_name in enumerate(files):\n",
    "            # Extract random predictions and actuals for one station\n",
    "            predictions = batch_predictions[:, idx, :].flatten()\n",
    "            labels = batch_labels[:, idx, :].flatten().cpu().numpy()\n",
    "\n",
    "            # Store data for plotting\n",
    "            station_data[station_name]['predictions'].extend(predictions)\n",
    "            station_data[station_name]['labels'].extend(labels)\n",
    "\n",
    "# Plot predicted vs actual values for each station\n",
    "fig, axs = plt.subplots(nrows=1, ncols=5, figsize=(15, 10))  # Adjust subplot layout to 1 row\n",
    "for ax, station_name in zip(axs, files):\n",
    "    preds = np.array(station_data[station_name]['predictions'])\n",
    "    actuals = np.array(station_data[station_name]['labels'])\n",
    "\n",
    "    lims = [0, 1]\n",
    "    \n",
    "    ax.scatter(preds, actuals, alpha=0.1)\n",
    "    ax.plot(lims, lims, 'r--', lw=2)  # Change reference line to red\n",
    "    ax.set_aspect('equal', adjustable='box')  # Ensure that axes are equally scaled\n",
    "    ax.set_xlim(lims)\n",
    "    ax.set_ylim(lims)\n",
    "    ax.set_title(f'Predicted vs Actual for {station_name}')\n",
    "    ax.set_xlabel('Predicted Values')\n",
    "    ax.set_ylabel('Actual Values')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debd502f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "best_model.eval()\n",
    "\n",
    "# Data containers for each station\n",
    "station_data = {name: {'predictions': [], 'labels': []} for name in files}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_data, batch_labels in test_loader:\n",
    "        # Assuming your reshape_to_batches handles batch data appropriately\n",
    "        batch_labels = reshape_to_batches(batch_labels, batch_data.batch)\n",
    "        batch_data = batch_data.cuda()\n",
    "        batch_labels = batch_labels.cuda().float()\n",
    "\n",
    "        # Generate random predictions that match the shape of batch_labels\n",
    "        batch_predictions = torch.rand_like(batch_labels)  # Generate random predictions directly as a torch tensor\n",
    "\n",
    "        for idx, station_name in enumerate(files):\n",
    "            # Extract random predictions and actuals for one station\n",
    "            predictions = batch_predictions[:, idx, :].flatten()\n",
    "            labels = batch_labels[:, idx, :].flatten()\n",
    "\n",
    "            # Store data for plotting and MSE calculation\n",
    "            station_data[station_name]['predictions'].extend(predictions.cpu().numpy())\n",
    "            station_data[station_name]['labels'].extend(labels.cpu().numpy())\n",
    "\n",
    "# Calculate and print MSE for each station\n",
    "for station_name in files:\n",
    "    preds = torch.tensor(station_data[station_name]['predictions'])\n",
    "    actuals = torch.tensor(station_data[station_name]['labels'])\n",
    "\n",
    "    mse = F.mse_loss(preds, actuals)\n",
    "    print(\"Mean Squared Error for\", station_name, \":\\t\", mse.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4494915e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
